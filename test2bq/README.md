# KSQL + Connect solution for data to BQ

POC with local containers - step by step

## Spin-up Confluent Platform

Compose and also build the connect docker image with kcbq while we're at it:

```bash
docker-compose up -d --build
```

## Create topic

```bash
kafka-topics \
    --zookeeper localhost:2181 \
    --create \
    -partitions 1 \
    --replication-factor 1 \
    --topic bq-sink-test
```

## Set up streams via KSQL

This can be done via control center/lenses or via REST API

1. Create stream from JSON

    TODO: consider partitioning

    NOTE: KSQL and Avro have a propensity to be case-agnostic,
    so it's easier to just stick to capitals...

    ```sql
    CREATE STREAM BQ_SINK_TEST
    (
        foo VARCHAR,
        bar VARCHAR,
        datetime VARCHAR,
        lat DOUBLE,
        lon DOUBLE,
        boo STRUCT<
            id VARCHAR>
    )
    WITH
    (
        KAFKA_TOPIC = 'bq-sink-test',
        VALUE_FORMAT = 'JSON'
    );
    ```

2. Create AVRO stream from the first one, flattening the nested fields and parsing timestamps

    ```sql
    CREATE STREAM BQ_SINK_TEST_AVRO
    WITH
    (
        VALUE_FORMAT = 'AVRO'
    )
    AS SELECT
        foo AS `foo`,
        boo AS `bar`,
        datetime AS `datetime`,
        ROUND(STRINGTOTIMESTAMP(dateTime, 'yyyy-MM-dd''T''HH:mm:ss.nnnnnn') * 0.001) AS `datetime_utc`,
        lat AS `lat`,
        lon AS `lon`,
        boo->id AS `boo_id`
    FROM
        BQ_SINK_TEST;
    ```

3. Configure connector

    Connector config (see `kcbq_config.json`):

    ```json
    {
        "name": "bq-sink-test",
        "config": {
            "tasks.max": "1",
            "connector.class": "com.wepay.kafka.connect.bigquery.BigQuerySinkConnector",
            "errors.log.enable": "true",
            "errors.log.include.messages": "true",
            "topics": "bq_sink_test_avro",
            "project": "phoenix-164312",
            "datasets": ".*=onefleet",
            "schemaRetriever": "com.wepay.kafka.connect.bigquery.schemaregistry.schemaretriever.SchemaRegistrySchemaRetriever",
            "schemaRegistryLocation": "http://schema-registry:8081",
            "keyfile": "/usr/share/application_credentials.json",
            "sanitizeTopics": true,
            "autoCreateTables": true
        }
    }
    ```

    Configure distributed connector:

    ```bash
    curl -X POST -H "Content-Type: application/json" \
        --data @kcbq_config.json \
        http://localhost:8083/connectors
    ```

    For updating the existing config you can use `PUT` on the `/connectors/connector-name/config` endpoint.

## Send some messages over

```bash
python3 dummy_producer/dummy_producer.py -n 5 --topic bq-sink-test
```

## Bigquery schema

1. Since KSQL Avro is case-agnostic, the autogenerated schemas
of both the topics and the tables shall be CAPITAL_CASE.
A bit annoying, but the only way to get around it is
to manually force the field names in the AVRO topic
to be mixed case, though then KSQL becomes confused
and I was not able to completely un-confuse it.
It would not read the manual message timestamp-field
from the message payload if the name of the field was not CAPITAL_CASE :(

2. Also, I haven't figured out yet how to gracefully make
KSQL generate correct AVRO for datetimes.
As it is it just writes MICROS as a long int.
KCBQ then just creates a table with an integer column. Better than just string, but still...

    We can, of course, make the schema manually:

    ```bash
    bq mk \
        --time_partitioning_type=DAY \
        onefleet.BQ_SINK_TEST_AVRO \
        bq_schema_with_timestamp.json
    ```

    But then also BQ expects milliseconds as an integer,
so we have to truncate the MICROS and loose precision.

## Kafka: Useful Stuff

To list consumer groups

```bash
kafka-consumer-groups.sh \
    --bootstrap-server BOOTSTRAP_SERVER_ADDRESS:17072 \
    --command-config consumer.properties \
    --list
```

To view details for a single group:

```bash
kafka-consumer-groups.sh \
    --bootstrap-server BOOTSTRAP_SERVER_ADDRESS:17072 \
    --command-config consumer.properties \
    --group test-group \
    --describe
```

To list current members of a group:

```bash
kafka-consumer-groups.sh \
    --bootstrap-server BOOTSTRAP_SERVER_ADDRESS:17072 \
    --command-config consumer.properties \
    --group test-group \
    --describe \
    --members
```

To reset an offset for a group, you can issue:

```bash
kafka-consumer-groups.sh \
    --bootstrap-server BOOTSTRAP_SERVER_ADDRESS:17072 \
    --command-config consumer.properties \
    --group test-group \
    --topic test-topic \
    --reset-offsets \
    --to-earliest \
    --execute
```
